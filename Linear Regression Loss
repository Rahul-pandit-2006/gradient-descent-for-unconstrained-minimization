# Data generation
set.seed(42)
n <- 100
x_data <- rnorm(n, mean = 1, sd = sqrt(2))
y_data <- rnorm(n, mean = 2 + 3 * x_data, sd = sqrt(5))

# Loss function
compute_loss <- function(x, y, beta) {
  n <- length(y)
  X <- cbind(1, x)  # Add intercept term
  residuals <- y - X %*% beta
  loss_val <- (1 / (2 * n)) * sum(residuals^2)
  return(loss_val)
}

# Gradient of the loss function
compute_gradient <- function(x, y, beta) {
  n <- length(y)
  X <- cbind(1, x)
  residuals <- y - X %*% beta
  grad_val <- -(1 / n) * t(X) %*% residuals
  return(grad_val)
}

# Gradient Descent implementation
gradient_descent <- function(x, y, beta_init, step_size, epsilon, max_iter = 10000) {
  beta <- beta_init
  loss_history <- c(compute_loss(x, y, beta))
  beta_history <- matrix(beta_init, nrow = 1)
  iter <- 0
  converged <- FALSE
  
  while (!converged && iter < max_iter) {
    grad <- compute_gradient(x, y, beta)
    beta <- beta - step_size * grad
    loss_val <- compute_loss(x, y, beta)
    
    loss_history <- c(loss_history, loss_val)
    beta_history <- rbind(beta_history, as.vector(beta))
    
    # Check gradient norm for convergence
    if (sqrt(sum(grad^2)) < epsilon) {
      converged <- TRUE
    }
    
    iter <- iter + 1
  }
  
  return(list(
    beta = beta,
    iterations = iter,
    loss_history = loss_history,
    beta_history = beta_history
  ))
}

# Run gradient descent
initial_beta <- c(0, 0)
learning_rate <- 0.01
threshold <- 1e-6

gd_result <- gradient_descent(
  x = x_data,
  y = y_data,
  beta_init = initial_beta,
  step_size = learning_rate,
  epsilon = threshold
)

# Print final results
cat("Final Beta Coefficients:\n")
print(gd_result$beta)
cat("Total Iterations:", gd_result$iterations, "\n")

# Plot Loss vs Iterations
plot(gd_result$loss_history, type = "l", col = "blue",
     main = "Loss vs Iterations (Linear Regression)",
     xlab = "Iteration", ylab = "Loss")
