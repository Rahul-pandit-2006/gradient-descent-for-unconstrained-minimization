rosen_loss_fn <- function(theta_vec) {
  return((1 - theta_vec[1])^2 + 100 * (theta_vec[2] - theta_vec[1]^2)^2)
}

rosen_grad_fn <- function(theta_vec) {
  grad_1 <- -2 * (1 - theta_vec[1]) - 400 * theta_vec[1] * (theta_vec[2] - theta_vec[1]^2)
  grad_2 <- 200 * (theta_vec[2] - theta_vec[1]^2)
  return(c(grad_1, grad_2))
}

gradient_descent_rosenbrock <- function(start_theta, learning_rate, tolerance, max_iter = 10000) {
  theta_current <- start_theta
  loss_current <- rosen_loss_fn(theta_current)
  loss_history <- c(loss_current)
  iteration <- 0

  repeat {
    gradient <- rosen_grad_fn(theta_current)
    theta_next <- theta_current - learning_rate * gradient
    loss_next <- rosen_loss_fn(theta_next)
    loss_history <- c(loss_history, loss_next)

    if (sqrt(sum(gradient^2)) < tolerance || iteration >= max_iter) break

    theta_current <- theta_next
    iteration <- iteration + 1
  }

  if (iteration >= max_iter) warning("Maximum iterations reached before convergence.")

  return(list(
    theta_final = theta_current,
    iterations = iteration,
    loss_trajectory = loss_history
  ))
}

result_rosen <- gradient_descent_rosenbrock(c(1, -1), learning_rate = 0.001, tolerance = 1e-6)

plot(result_rosen$loss_trajectory, type = "l", col = "blue",
     xlab = "Iteration", ylab = "Loss",
     main = "Loss vs Iterations (Rosenbrock Function)")
