# Load data
data <- read.csv("C:\\Users\\adi17\\Downloads\\synthetic data - problem-2.csv")

# Extract features and label
feature_1 <- data$x1
feature_2 <- data$x2
target <- data$y

# Negative Log-Likelihood (Loss) for Logistic Regression
compute_logistic_loss <- function(x1, x2, y, beta) {
  X <- cbind(x1, x2)
  linear_term <- X %*% beta
  neg_log_likelihood <- -sum(y * linear_term - log(1 + exp(linear_term)))
  return(neg_log_likelihood)
}

# Gradient of the Logistic Loss
compute_logistic_gradient <- function(x1, x2, y, beta) {
  X <- cbind(x1, x2)
  linear_term <- X %*% beta
  prob <- 1 / (1 + exp(-linear_term))  # Sigmoid
  gradient <- -t(X) %*% (y - prob)
  return(as.vector(gradient))
}

# Gradient Descent for Logistic Regression
gradient_descent_logistic <- function(x1, x2, y, beta_init, step_size, epsilon, max_iter = 10000) {
  beta <- beta_init
  loss_history <- c(compute_logistic_loss(x1, x2, y, beta))
  iter <- 0
  converged <- FALSE
  
  while (!converged && iter < max_iter) {
    grad <- compute_logistic_gradient(x1, x2, y, beta)
    beta <- beta - step_size * grad
    loss_val <- compute_logistic_loss(x1, x2, y, beta)
    loss_history <- c(loss_history, loss_val)
    
    # Convergence based on gradient norm
    if (sqrt(sum(grad^2)) < epsilon) {
      converged <- TRUE
    }
    
    iter <- iter + 1
  }
  
  if (!converged) warning("Maximum iterations reached before convergence.")
  
  return(list(
    beta = beta,
    iterations = iter,
    loss_history = loss_history
  ))
}

# Initialize parameters
initial_beta <- c(0, 0)
learning_rate <- 0.05
threshold <- 1e-5

# Run gradient descent
logistic_result <- gradient_descent_logistic(
  x1 = feature_1,
  x2 = feature_2,
  y = target,
  beta_init = initial_beta,
  step_size = learning_rate,
  epsilon = threshold
)

# Output results
cat("Final Beta Coefficients:\n")
print(logistic_result$beta)
cat("Total Iterations:", logistic_result$iterations, "\n")

# Plot Loss over Iterations
plot(logistic_result$loss_history, type = "l", col = "darkred",
     main = "Loss vs Iterations (Logistic Regression)",
     xlab = "Iteration", ylab = "Negative Log-Likelihood")
