
# Load data

sample_data <- read.csv("C:\\Users\\adi17\\Downloads\\synthetic data - problem-4.csv")
observations <- sample_data$x

# Negative Log-Likelihood

normal_neg_log_likelihood <- function(data, theta) {
  mu <- theta[1]
  sigma2 <- theta[2]
  n <- length(data)
  
  if (sigma2 <= 0) return(Inf)
  
  nll <- (n / 2) * log(sigma2) + sum((data - mu)^2) / (2 * sigma2)
  return(nll)
}


# Gradient of NLL

normal_nll_gradient <- function(data, theta) {
  mu <- theta[1]
  sigma2 <- theta[2]
  n <- length(data)
  
  if (sigma2 <= 0) return(c(NA, NA))  # Prevent invalid operations
  
  grad_mu <- -sum(data - mu) / sigma2
  grad_sigma2 <- (n / (2 * sigma2)) - sum((data - mu)^2) / (2 * sigma2^2)
  
  return(c(grad_mu, grad_sigma2))
}


# Gradient Descent Procedure
gradient_descent_normal_mle <- function(data, theta_start, learning_rate, tolerance, max_iter = 10000) {
  theta_current <- theta_start
  loss_current <- normal_neg_log_likelihood(data, theta_current)
  loss_history <- c(loss_current)
  iteration <- 0
  
  repeat {
    grad <- normal_nll_gradient(data, theta_current)
    
    if (any(is.na(grad))) stop("Gradient returned NA due to invalid variance.")
    
    # Update parameters
    theta_next <- theta_current - learning_rate * grad
    
    # Ensure positive variance
    if (theta_next[2] <= 0) theta_next[2] <- 1e-6
    
    loss_next <- normal_neg_log_likelihood(data, theta_next)
    loss_history <- c(loss_history, loss_next)
    
    # Convergence check (gradient norm)
    if (sqrt(sum(grad^2)) < tolerance || iteration >= max_iter) break
    
    theta_current <- theta_next
    iteration <- iteration + 1
  }
  
  if (iteration >= max_iter) warning("Reached maximum iterations before convergence.")
  
  return(list(
    theta_final = theta_current,
    iterations = iteration,
    loss_trajectory = loss_history
  ))
}


# Initialization & Execution
initial_theta <- c(mu = 0, sigma2 = 1)
learning_rate <- 0.01
epsilon <- 1e-5

mle_result <- gradient_descent_normal_mle(
  data = observations,
  theta_start = initial_theta,
  learning_rate = learning_rate,
  tolerance = epsilon
)

# Results

cat("Estimated Mean (μ):", mle_result$theta_final[1], "\n")
cat("Estimated Variance (σ²):", mle_result$theta_final[2], "\n")
cat("Total Iterations:", mle_result$iterations, "\n")


# Plot: Loss vs Iterations
plot(mle_result$loss_trajectory, type = "l", col = "blue",
     main = "Convergence of Negative Log-Likelihood",
     xlab = "Iteration", ylab = "Negative Log-Likelihood")
