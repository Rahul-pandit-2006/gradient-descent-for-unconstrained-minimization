
# Quadratic Loss Function

quad_objective_loss <- function(theta_vec) {
  val <- 2 * theta_vec[1]^2 + 4 * theta_vec[2]^2 - 4 * theta_vec[1] - 8 * theta_vec[2]
  return(val)
}


# Gradient of the Quadratic Function
quad_objective_grad <- function(theta_vec) {
  grad_1 <- 4 * theta_vec[1] - 4
  grad_2 <- 8 * theta_vec[2] - 8
  return(c(grad_1, grad_2))
}

# Gradient Descent Procedure

gradient_descent_quadratic <- function(initial_theta, learning_rate, tolerance, max_iter = 10000) {
  theta_current <- initial_theta
  loss_current <- quad_objective_loss(theta_current)
  loss_history <- c(loss_current)
  iteration <- 0
  
  repeat {
    grad <- quad_objective_grad(theta_current)
    theta_next <- theta_current - learning_rate * grad
    loss_next <- quad_objective_loss(theta_next)
    
    loss_history <- c(loss_history, loss_next)
    
    # Convergence check using gradient norm
    if (sqrt(sum(grad^2)) < tolerance || iteration >= max_iter) break
    
    theta_current <- theta_next
    iteration <- iteration + 1
  }
  
  if (iteration >= max_iter) warning("Reached maximum iterations before convergence.")
  
  return(list(
    theta_final = theta_current,
    iterations = iteration,
    loss_trajectory = loss_history
  ))
}


# Run the Gradient Descent

initial_theta <- c(0, 0)
result <- gradient_descent_quadratic(initial_theta, learning_rate = 0.1, tolerance = 1e-6)


# Print Results

cat("Optimal theta values:\n")
cat("x1 =", result$theta_final[1], "\n")
cat("x2 =", result$theta_final[2], "\n")
cat("Iterations:", result$iterations, "\n")


# Plot: Loss vs Iteration

plot(result$loss_trajectory, type = "l", col = "red",
     xlab = "Iteration", ylab = "Loss",
     main = "Convergence of Quadratic Loss")
